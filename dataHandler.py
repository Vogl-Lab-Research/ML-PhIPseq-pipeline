import os
from pathlib import Path
import pandas as pd
pd.set_option("future.no_silent_downcasting", True)
import numpy as np
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import xgboost as xgb
#random forest
from peptidesFilter import PrevalenceFilter, EntropyFilter, CorrelationFilter
import yaml
import importlib

class Config:
    def __init__(self,
                 config_file,
                 library_dir = None, metadata_dir = None, data_dir = None,
                 project = None, library_metadata = None, group_tests = None,
                 data_types=None,
                 col_sample_name=None, col_target=None, col_predict=None,
                 random_state=None, extra_features_to_include=None,
                 entropy_threshold=None,
                 prevalence_thresholds_min=None, prevalence_thresholds_max=None,
                 param_grid=None,
                 with_oligos_options=None, with_additional_features_options=None,
                 with_run_plates_options=None, filter_by_entropy=None, filter_by_correlation=None,
                 subgroups_to_name=None, subgroups_order=None, subgroups_to_include=None,
                 estimators_info=None, subgroups_colors=None,
                 filters_metadata=None, combined_filters_metadata=None,

                 cv_method=None, split_train_test=None, train_size=None,
                 tuning_n_iter=None, tuning_k=None, tuning_parameters=None,
                 external_set=None, compute_feature_importance=None, k=None, imputed=None, transposed=None,
                 fillna=None, libraries_prefixes=None, return_train=None, return_test=None
                 ):
        # Mandatory to set
        self.library_dir = Path(library_dir) if isinstance(library_dir, str) else None
        self.metadata_dir = Path(metadata_dir) if isinstance(metadata_dir, str) else None
        self.data_dir = Path(data_dir) if isinstance(data_dir, str) else None
        self.project = project if isinstance(project, str) else None
        self.lib_meta_data = library_metadata if isinstance(library_metadata, str) else None
        self.group_tests = group_tests if isinstance(group_tests, str) else None

        #default values
        self.col_sample_name = col_sample_name if isinstance(col_sample_name, str) else 'SampleName'
        self.col_target = col_target if isinstance(col_target, str) else 'group_test'
        self.col_predict = col_predict if isinstance(col_predict, str) else 'class1_proba'
        self.sex_encoding = {'F': 0, 'M': 1}
        self.random_state = random_state if isinstance(random_state, int) else 420
        self.extra_features_to_include = extra_features_to_include if (isinstance(extra_features_to_include, list) and
                                                                       all(isinstance(item, str) for item in
                                                                           extra_features_to_include)) else ['Sex',
                                                                                                             'Age']
        self.data_types = data_types if (isinstance(data_types, list) and
                                         all(isinstance(item, str) for item in data_types)) else ['exist']

        self.entropy_threshold = entropy_threshold if isinstance(entropy_threshold, float) else 0.4

        self.prevalence_thresholds_min = prevalence_thresholds_min if (isinstance(prevalence_thresholds_min, list) and
                                                                       all(isinstance(item, (int, float)) for item in
                                                                           prevalence_thresholds_min)) else [5., 10.,
                                                                                                             20., 50.]
        self.prevalence_thresholds_max = prevalence_thresholds_max if (isinstance(prevalence_thresholds_max, list) and
                                                                       all(isinstance(item, (int, float)) for item in
                                                                           prevalence_thresholds_max)) else [95.]
        self.with_oligos_options = with_oligos_options if (isinstance(with_oligos_options, list) and
                                                           all(isinstance(item, bool) for item in
                                                               with_oligos_options)) else [True, False]
        self.with_additional_features_options = with_additional_features_options if (
                    isinstance(with_additional_features_options, list) and
                    all(isinstance(item, bool) for item in with_additional_features_options)) else [True, False]
        self.with_run_plates_options = with_run_plates_options if (isinstance(with_run_plates_options, list) and
                                                                   all(isinstance(item, bool) for item in
                                                                       with_run_plates_options)) else [False]
        self.filter_by_entropy = filter_by_entropy if (isinstance(filter_by_entropy, list) and
                                                       all(isinstance(item, bool) for item in filter_by_entropy)) else [False]
        self.filter_by_correlation = filter_by_correlation if (isinstance(filter_by_correlation, list) and
                                                               all(isinstance(item, bool) for item in
                                                                   filter_by_correlation)) else [False]
        self.subgroups_to_name = subgroups_to_name if isinstance(subgroups_to_name, dict) else {
            'all': 'Complete library', 'bloodtests': 'Blood tests',
            'is_ALIGENT': 'Aligent library', 'is_TWIST': 'Twist library', 'is_CORONA': 'Corona library',
            'is_PNP': 'Metagenomics\nantigens', 'is_auto': 'Human Autoantigens', 'is_patho': 'Pathogenic strains',
            'is_probio': 'Probiotic strains', 'is_IgA': 'Antibody-coated\nstrains',
            'is_MPA': 'Microbiota\nstrains', 'is_bac_flagella': 'Flagellins', 'is_infect': 'Infectious\npathogens',
            'is_EBV': 'Epstein-Barr\nVirus', 'is_toxin': 'Toxin',
            'is_phage': 'Phages', 'is_allergens': 'Allergens', 'is_influenza': 'Influenza',
            'is_EM': 'Microbiota\ngenes', 'signalp6_slow': 'Secreted proteins',
            'is_topgraph_new_&_old': 'Membrane proteins',
            'diamond_mmseqs_intersec_toxin': 'Predicted toxins',
            'is_IEDB_or_cntrl': 'IEDB/controls',
            'is_pos_cntrl': 'Positive control', 'is_neg_cntrl': 'Negative control', 'is_rand_cntrl': 'Random control'
        }
        self.subgroups_order = subgroups_order if (isinstance(subgroups_order, list) and
                                                   all(isinstance(item, str) for item in subgroups_order)) else \
            ['Complete library',
             'Aligent library', 'Twist library',  # 'Corona library',
             'Metagenomics\nantigens', 'Human Autoantigens', 'Pathogenic strains', 'Probiotic strains',
             'Antibody-coated\nstrains',
             'Microbiota\nstrains', 'Flagellins', 'Infectious\npathogens', 'Epstein-Barr\nVirus', 'Toxin',
             'Phages', 'Allergens', 'Influenza',
             'Microbiota\ngenes', 'Secreted proteins', 'Membrane proteins',
             'Predicted toxins',
             'IEDB/controls'
                , 'Positive control', 'Negative control', 'Random control'
             ]
        self.subgroups_to_include = subgroups_to_include if (isinstance(subgroups_to_include, list) and
                                                             all(isinstance(item, str) for item in
                                                                 subgroups_to_include)) else \
            [
                'all',
                'is_ALIGENT', 'is_TWIST',  # 'is_CORONA',
                'is_PNP', 'is_auto', 'is_patho', 'is_probio', 'is_IgA',
                'is_MPA', 'is_bac_flagella', 'is_infect', 'is_EBV', 'is_toxin',
                'is_phage', 'is_allergens', 'is_influenza',
                'is_EM', 'signalp6_slow', 'is_topgraph_new_&_old',
                'diamond_mmseqs_intersec_toxin',
                'is_IEDB_or_cntrl',
                'is_pos_cntrl', 'is_neg_cntrl', 'is_rand_cntrl'
            ]
        self.estimators_info = estimators_info if isinstance(estimators_info, dict) else {
            'XGBClassifier': {'estimator_class': xgb.XGBClassifier,
                              'estimator_kwargs': {'objective': 'binary:logistic',
                                                   'eval_metric': 'auc', 'random_state': self.random_state,
                                                   'nthread': 1, 'n_jobs': -1,
                                                   'n_estimators': 150, 'learning_rate': 0.1, "max_depth": 6
                                                   }}}
        self.param_grid = param_grid if isinstance(param_grid, dict) else {'XGBClassifier': {
            'n_estimators': [50, 100, 200, 500, 1000],
            'learning_rate': [0.01, 0.1, 0.3],  # Learning rate (eta)
            'max_depth': [4, 6, 8],  # Maximum tree depth for base learners
            # 'gamma': [0, 0.1, 0.3, 0.5],                              # Minimum loss reduction for a split
            'subsample': [0.6, 0.8, 1.0],                             # Subsample ratio of the training instances
            'colsample_bytree': [0.6, 0.8, 1.0],  # Subsample ratio of columns when constructing each tree
            # 'reg_alpha': [0, 0.1, 0.5, 1.0],                          # L1 regularization term
            'reg_lambda': [1, 1.5, 2, 3]  # L2 regularization term
            #,'booster': ['gbtree', 'dart']
        }}
        self.subgroups_colors = subgroups_colors if isinstance(subgroups_colors, dict) else dict(
            zip(self.subgroups_order, sns.color_palette()[0:4] + sns.color_palette("Set2", 12)[5:6] +
                sns.color_palette("Set3", 12)[9:10] + sns.color_palette()[8:10] +
                sns.color_palette()[5:6] + [sns.color_palette("Set2", 8)[i] for i in [1, 2, 6]] +
                sns.color_palette('Accent')[5:6] + sns.color_palette("Set3", 12)[0:1] +
                sns.color_palette("Set3", 12)[2:7] + sns.color_palette("Set2", 8)[0:1] +
                sns.color_palette('PRGn')[0:1] + sns.husl_palette(s=.4)))
        self.additional_features_only_color = 'black'
        self.run_plates_only_color = 'silver'
        self.additional_features_run_plates_only_color = 'gray'

        self.filters_metadata = filters_metadata if isinstance(filters_metadata,
                                                               dict) else None  # Default to an empty dict
        self.combined_filters_metadata = combined_filters_metadata if (isinstance(combined_filters_metadata, list) and
                                                                       all(isinstance(item, dict) for item in
                                                                           combined_filters_metadata)) else None  # Default to an empty list

        # Oligo Handler
        self.transposed = transposed if isinstance(transposed, bool) else True
        # Meta Handler
        self.imputed = imputed if isinstance(imputed, bool) else False
        # FeatureManager
        self.fillna = fillna if isinstance(fillna, bool) else False

        # Performance validator
        self.libraries_prefixes = libraries_prefixes if (isinstance(libraries_prefixes, list) and
                                                                       all(isinstance(item, str) for item in libraries_prefixes)) else ['agilent', 'corona2', 'twist']
        self.cv_method = cv_method if cv_method in ("loo", "kfold") else "kfold"
        self.split_train_test = split_train_test if isinstance(split_train_test, bool) else True
        self.compute_feature_importance = compute_feature_importance if isinstance(compute_feature_importance, bool) else False
        self.return_train = return_train if isinstance(return_train, bool) else True
        self.return_test = return_test if isinstance(return_test, bool) else False
        self.external_set = external_set if isinstance(external_set, bool) else False
        self.tuning_parameters = tuning_parameters if isinstance(tuning_parameters, bool) else True
        self.train_size = train_size if isinstance(train_size, (int, float)) else 0.8
        self.k = k if isinstance(k, int) else 10
        self.tuning_n_iter = tuning_n_iter if isinstance(tuning_n_iter, int) else 20
        self.tuning_k = tuning_k if isinstance(tuning_k, int) else 3

        # must be set explicitly
        self.config_file = config_file if isinstance(config_file, str) else None
        # overwrite parameters present in yaml file
        if self.config_file is not None:
            if not (self.config_file.endswith('.yaml') or self.config_file.endswith('.yml')):
                raise ValueError(
                    f"Config file '{self.config_file}' does not appear to be a YAML file (expected .yaml or .yml).")
            try:
                self.load_from_file(self.config_file)
            except yaml.YAMLError as e:
                raise ValueError(f"The config file '{self.config_file}' is not valid YAML. Error: {e}")
        else:
            raise ValueError("A YAML file must be provided")

        # Validate that mandatory attributes are set
        self.validate_mandatory_attributes()

        # derived values
        self.label_group_tests = "-".join(self.group_tests)
        self.group_label_encoding = {test: idx for idx, test in enumerate(self.group_tests)}
        self.predictions_dir = Path(self.data_dir) / 'Predictions'
        self.visualization_dir = Path(self.predictions_dir) / 'Figures'
        self.figures_dir = Path(self.visualization_dir) / f'figures_{self.label_group_tests}'

        # Ensure the directories exist
        if not os.path.exists(self.predictions_dir):
            os.makedirs(self.predictions_dir)
        if not os.path.exists(self.visualization_dir):
            os.makedirs(self.visualization_dir)
        if not os.path.exists(self.figures_dir):
            os.makedirs(self.figures_dir)

    def validate_mandatory_attributes(self):
        """Validate that mandatory attributes are set and not None."""
        mandatory_attributes = ['metadata_dir', 'data_dir', 'project', 'lib_meta_data', 'group_tests']
        for attr in mandatory_attributes:
            if getattr(self, attr) is None:
                raise ValueError(f"The mandatory attribute '{attr}' is not set or is None.")

    def set_attribute(self, attr_name, value):
        if hasattr(self, attr_name):
            current_value = getattr(self, attr_name, value)
            # Only perform type checking if the attribute already has a non-None value
            if current_value is not None and not isinstance(value, type(current_value)) and not isinstance(current_value, Path):
                raise TypeError(
                    f"Expected value of type {type(current_value)} for attribute '{attr_name}', but got {type(value)}")
            #setattr(self, attr_name, value)
            setattr(self, attr_name, Path(value) if attr_name == "metadata_dir" or attr_name == "data_dir" else value)

        else:
            raise AttributeError(f"Config has no attribute named '{attr_name}'")

    def process_estimators_info(self):
        """
        Converts any 'estimator_class' entries from strings to actual classes.
        """
        def get_class_from_string(class_path: str):
            module_name, class_name = class_path.rsplit('.', 1)
            module = importlib.import_module(module_name)
            return getattr(module, class_name)

        if hasattr(self, 'estimators_info') and isinstance(self.estimators_info, dict):
            for name, info in self.estimators_info.items():
                class_str = info.get('estimator_class')
                if class_str and isinstance(class_str, str):
                    info['estimator_class'] = get_class_from_string(class_str)

    def load_from_file(self, config_file):
        with open(config_file, 'r') as f:
            config_data = yaml.safe_load(f)

        #Only update attributes that exist in config_data, leaving all other defaults intact.
        for key, value in config_data.items():
            # If the attribute exists in the class, override it.
            # This ensures we only update keys present in the config.
            self.set_attribute(key, value)

        self.process_estimators_info()



    def _set_string_attribute(self, attr_name, value):
        if not isinstance(value, str):
            raise ValueError(f"{attr_name} must be a string.")
        setattr(self, attr_name, value)

        # Special case: if DATA_DIR is updated, automatically set PREDICTIONS_DIR
        if attr_name == 'data_dir':
            self.predictions_dir = Path(self.data_dir) / 'Predictions'

    def _set_list_string_attribute(self, attr_name, value):
        if not isinstance(value, list) or not all(isinstance(item, str) for item in value):
            raise ValueError(f"{attr_name} must be a list of strings.")
        setattr(self, attr_name, value)

    def _set_list_boolean_attribute(self, attr_name, value):
        if not isinstance(value, list) or not all(isinstance(item, bool) for item in value):
            raise ValueError(
                f"{attr_name} must be a list of booleans and can only contain [True, False], [True], or [False].")
        if set(value) not in [{True}, {False}, {True, False}]:
            raise ValueError(f"{attr_name} must only be [True, False], [True], or [False].")
        setattr(self, attr_name, value)

    def _set_dict_string_attribute(self, attr_name, value):
        if not isinstance(value, dict):
            raise ValueError(f"The attribute '{attr_name}' must be a dictionary.")

        # Check if all keys and values are strings
        if not all(isinstance(k, str) and isinstance(v, str) for k, v in value.items()):
            raise ValueError(f"The attribute '{attr_name}' must be a dictionary with string keys and string values.")
        setattr(self, attr_name, value)

    def _set_list_of_dict_string_attribute(self, attr_name, value):
        if not isinstance(value, list):
            raise ValueError(f"The attribute '{attr_name}' must be a list of dictionaries.")

        if not all(isinstance(item, dict) for item in value):
            raise ValueError(f"All elements in the attribute '{attr_name}' must be dictionaries.")

        # Check if all keys and values in each dictionary are strings
        for dictionary in value:
            if not all(isinstance(k, str) and isinstance(v, str) for k, v in dictionary.items()):
                raise ValueError(
                    f"All dictionaries in the attribute '{attr_name}' must have string keys and string values.")
        setattr(self, attr_name, value)

    # Using the helper function for each setter
    def set_metadata_dir(self, metadata_dir):
        self._set_string_attribute('metadata_dir', metadata_dir)

    def set_data_dir(self, data_dir):
        self._set_string_attribute('data_dir', data_dir)

    def set_visualization_dir(self, visualization_dir):
        self._set_string_attribute('visualization_dir', visualization_dir)
        self.figures_dir = Path(visualization_dir) / f'figures_{self.label_group_tests}'
        os.makedirs(self.figures_dir, exist_ok=True)

    def set_project(self, project):
        self._set_string_attribute('project', project)

    def set_lib_meta_data(self, lib_meta_data):
        self._set_string_attribute('lib_meta_data', lib_meta_data)

    def set_column_sample_name(self, col_sample_name):
        self._set_string_attribute('col_sample_name', col_sample_name)

    def set_column_target(self, col_target):
        self._set_string_attribute('col_target', col_target)

    def set_column_predict(self, col_predict):
        self._set_string_attribute('col_predict', col_predict)

    def set_group_tests(self, group_tests):
        self._set_list_string_attribute('group_tests', group_tests)
        self.label_group_tests = "-".join(group_tests)
        self.group_label_encoding = {test: idx for idx, test in enumerate(group_tests)}

    def set_extra_features(self, extra_features):
        self._set_list_string_attribute('extra_features_to_include', extra_features)

    def set_data_types(self, data_types):
        self._set_list_string_attribute('data_types', data_types)

    def set_prevalence_thresholds_min(self, prevalence_thresholds_min):
        if not isinstance(prevalence_thresholds_min, list):
            raise ValueError(f"prevalence_thresholds must be a list of integers/float between 0-100.")
        setattr(self, 'prevalence_thresholds_min', prevalence_thresholds_min)

    def set_prevalence_thresholds_max(self, prevalence_thresholds_max):
        if not isinstance(prevalence_thresholds_max, list):
            raise ValueError(f"prevalence_thresholds must be a list of integers/float between 0-100.")
        setattr(self, 'prevalence_thresholds_max', prevalence_thresholds_max)

    def set_with_oligos_options(self, value):
        self._set_list_boolean_attribute('with_oligos_options', value)

    def set_with_additional_features_options(self, value):
        self._set_list_boolean_attribute('with_additional_features_options', value)

    def set_with_run_plates_options(self, value):
        self._set_list_boolean_attribute('with_run_plates_options', value)

    def set_filter_by_entropy(self, value):
        self._set_list_boolean_attribute('filter_by_entropy', value)

    def set_filter_by_correlation(self, value):
        self._set_list_boolean_attribute('filter_by_correlation', value)

    def set_filters_metadata(self, value):
        self._set_dict_string_attribute('filters_metadata', value)

    def set_combined_filters_metadata(self, value):
        self._set_list_of_dict_string_attribute('combined_filters_metadata', value)

    def get_attribute(self, attr_name):
        if hasattr(self, attr_name):
            return getattr(self, attr_name)
        else:
            raise AttributeError(f"Config has no attribute named '{attr_name}'")

class MetadataHandler:
    def __init__(self, config):
        self.config = config
        self.imputed = self.config.imputed #imputed if isinstance(imputed, bool) else False

    def set_imputed(self, imputed):
        if isinstance(imputed, bool):
            self.imputed = imputed
        else:
            raise ValueError("The imputed parameter must be a boolean.")

    @staticmethod
    def filter_metadata(ind_meta, filters_metadata):
        """
        Filters the metadata DataFrame based on specified filter conditions.
        """
        if not isinstance(filters_metadata, dict):
            raise ValueError(f"Value '{filters_metadata}' must be a dictionary "
                             f"with key as column name and value as the attributes to subset")

        for column, value in filters_metadata.items():
            if column in ind_meta.columns:
                if isinstance(value, list):
                    ind_meta = ind_meta[ind_meta[column].isin(value)]
                else:
                    ind_meta = ind_meta[ind_meta[column] == value]
            else:
                raise ValueError(f"Column '{column}' does not exist in the metadata.")

        return ind_meta

    @staticmethod
    def apply_combined_filters_metadata(ind_meta, combined_filters_metadata):
        """
        Applies combined filtering conditions to the metadata DataFrame.
        """
        if not isinstance(combined_filters_metadata, list):
            raise ValueError(f"Value '{combined_filters_metadata}' must be a list of dictionaries. "
                       f"Each dictionary with key as column name and value as the attributes to subset")

        combined_filter = pd.Series([False] * len(ind_meta), index=ind_meta.index)
        for condition in combined_filters_metadata:
            temp_filter = pd.Series([True] * len(ind_meta), index=ind_meta.index)
            for column, value in condition.items():
                if column in ind_meta.columns:
                    if isinstance(value, list):
                        temp_filter &= ind_meta[column].isin(value)
                    else:
                        temp_filter &= (ind_meta[column] == value)
                else:
                    raise ValueError(f"Column '{column}' does not exist in the metadata.")
            combined_filter |= temp_filter
        return ind_meta[combined_filter]

    def get_individuals_metadata_df(self):
        # Build the base file path without extension.
        metadata_path = Path(self.config.metadata_dir) / f'{self.config.project}_metadata'

        # Check for the excel version first.
        excel_file = metadata_path.with_suffix('.xlsx')
        csv_file = metadata_path.with_suffix('.csv')

        if excel_file.exists():
            ind_meta = pd.read_excel(excel_file, sheet_name=0, index_col=self.config.col_sample_name)
        elif csv_file.exists():
            ind_meta = pd.read_csv(csv_file, index_col=self.config.col_sample_name, low_memory=False)
        else:
            raise ValueError("No Sample metadata file found. Expected a file with suffix '.xlsx' or '.csv'.")

        # Drop unnamed numeric columns
        unnamed_col = ind_meta.columns[ind_meta.columns.str.match(r'^Unnamed')].tolist()
        if unnamed_col:
            # Check if the unnamed column contains a numeric range from 0 to number of rows - 1
            col_name = unnamed_col[0]
            if ind_meta[col_name].dtype.kind in 'iufc' and (ind_meta[col_name] == range(len(ind_meta))).all():
                ind_meta = ind_meta.drop(columns=[col_name])

        # Drop columns with 'Sample' or 'barcode'
        columns_to_drop = ind_meta.columns[ind_meta.columns.str.contains(r'Sample|barcode', case=False)].tolist()
        if columns_to_drop:
            ind_meta = ind_meta.drop(columns=columns_to_drop)

        #if self.config.group_tests  is None:
        #    self.config.group_tests = ind_meta[self.config.COL_TARGET].unique()
        #ind_meta = ind_meta[ind_meta[self.config.COL_TARGET].isin(self.config.group_tests )]

        # Rename 'Gender' to 'Sex' and encode values if necessary
        if 'Gender' in ind_meta.columns and not 'Sex' in ind_meta.columns:
            ind_meta = ind_meta.rename(columns={'Gender': 'Sex'})
        if 'Sex' in ind_meta.columns and ind_meta['Sex'].isin(['F', 'M']).any():
            ind_meta['Sex'] = ind_meta['Sex'].replace(self.config.sex_encoding).astype(int)

        if self.config.filters_metadata:
            ind_meta = self.filter_metadata(ind_meta, self.config.filters_metadata)
        # Apply combined conditions if specified
        elif self.config.combined_filters_metadata:
            ind_meta = self.apply_combined_filters_metadata(ind_meta, self.config.combined_filters_metadata)


        return ind_meta

    def get_additional_features_df(self, **impute_kwargs):
        ind_meta_df = self.get_individuals_metadata_df()[self.config.extra_features_to_include]
        if self.imputed:
            imputer = SimpleImputer(**impute_kwargs)
            ind_meta_df = pd.DataFrame(data=imputer.fit_transform(ind_meta_df), columns=ind_meta_df.columns,
                                       index=ind_meta_df.index)
        return ind_meta_df

    def get_run_plates_df(self):
        meta_df = self.get_individuals_metadata_df().reset_index()
        meta_df['Run_Plate'] = meta_df[self.config.col_sample_name].str.extract(r'(^R\d+P\d+)')
        run_plates_df = meta_df[[self.config.col_sample_name, 'Run_Plate']]

        # Perform one-hot encoding on the extracted Run_Plate column
        return pd.get_dummies(run_plates_df, columns=['Run_Plate'], prefix='Run_Plate', dtype=int).set_index(
            [self.config.col_sample_name])

    def get_additional_features_run_plates_df(self, **impute_kwargs):
        ind_meta_df = self.get_additional_features_df(**impute_kwargs)
        run_plate_df = self.get_run_plates_df()
        return pd.merge(ind_meta_df, run_plate_df, left_on=self.config.col_sample_name, right_on=self.config.col_sample_name)

class OligosHandler:
    def __init__(self, config, data_type = None):
        self.config = config
        self.data_type = data_type if isinstance(data_type, str) else 'exist'
        self.transposed = self.config.transposed

    def set_transposed(self, transposed):
        if isinstance(transposed, bool):
            self.transposed = transposed
        else:
            raise ValueError("transposed must be a boolean.")

    def set_data_type(self, data_type):
        if isinstance(data_type, str):
            self.data_type = data_type
        else:
            raise ValueError("data_type must be a str.")

    def get_oligos_df(self):
        if self.data_type not in ['fold', 'exist', 'p_val']:
            raise ValueError(f"Invalid file type: {self.data_type}. Expected types: 'fold', 'exist', or 'p_val'.")
        oligos_df = pd.read_csv(Path(self.config.data_dir) / f"{self.data_type}.csv", index_col=0, low_memory=False)
        return oligos_df.T if self.transposed else oligos_df

    def get_oligos_metadata_df(self):
        file_path = Path(self.config.metadata_dir) / self.config.lib_meta_data
        if file_path.suffix == '.pkl':
            df = pd.read_pickle(file_path)
        elif file_path.suffix == '.csv':
            df = pd.read_csv(file_path)
        else:
            raise ValueError("The LIB_META_DATA must be a '.pkl' or '.csv' filename")
        return df.sort_index(ascending=True, inplace=False)


class FeatureManager:
    def __init__(self, config, metadata_handler, oligos_handler,
                 subgroup = None, group_oligos = None,
                 with_oligos = None, with_additional_features = None, with_run_plates = None,
                 filter_by_correlation = None, filter_by_entropy = None,
                 entropy_threshold = None, prevalence_threshold_max = None, prevalence_threshold_min = None):

        self.config = config
        self.metadata_handler = metadata_handler
        self.oligos_handler = oligos_handler

        self.le_classes = None

        self.fillna = self.config.fillna #fillna if isinstance(fillna, bool) else False

        self.with_oligos = with_oligos if isinstance(with_oligos, bool) else True
        self.with_additional_features = with_additional_features if isinstance(with_additional_features, bool) else False
        self.with_run_plates = with_run_plates if isinstance(with_run_plates, bool) else False
        self.group_oligos = group_oligos if isinstance(group_oligos, bool) else False

        self.filter_by_correlation = filter_by_correlation if isinstance(filter_by_correlation, bool) else False
        self.filter_by_entropy = filter_by_entropy if isinstance(filter_by_entropy, bool) else False
        if self.filter_by_entropy:
            if entropy_threshold is not None and isinstance(entropy_threshold, float):
                self.entropy_threshold = entropy_threshold
            elif config.entropy_threshold is not None:
                self.entropy_threshold = config.entropy_threshold
            else:
                self.entropy_threshold = 0.4
        self.prevalence_threshold_min = prevalence_threshold_min if isinstance(prevalence_threshold_min, (int, float, np.integer, np.floating)) else 2.0
        self.prevalence_threshold_max = prevalence_threshold_max if isinstance(prevalence_threshold_max, (int, float, np.integer, np.floating)) else 95.0

        self.subgroup = subgroup if isinstance(subgroup, str) else 'all'

    def _set_bool_attribute(self, attr_name, value):
        if not isinstance(value, bool):
            raise ValueError(f"{attr_name} must be a boolean.")
        setattr(self, attr_name, value)


    def set_fillna(self, fillna):
        self._set_bool_attribute('fillna', fillna)

    def set_with_oligos(self, with_oligos):
        self._set_bool_attribute('with_oligos', with_oligos)

    def set_with_additional_features(self, with_additional_features):
        self._set_bool_attribute('with_additional_features', with_additional_features)

    def set_with_run_plates(self, with_run_plates):
        self._set_bool_attribute('with_run_plates', with_run_plates)

    def set_filter_by_correlation(self, filter_by_correlation):
        self._set_bool_attribute('filter_by_correlation', filter_by_correlation)

    def set_filter_by_entropy(self, filter_by_entropy):
        self._set_bool_attribute('filter_by_entropy', filter_by_entropy)

    def set_group_oligos(self, group_oligos):
        self._set_bool_attribute('group_oligos', group_oligos)

    def set_entropy_threshold(self, entropy_threshold):
        if isinstance(entropy_threshold, float):
            self.entropy_threshold = entropy_threshold
        else:
            raise ValueError("The entropy_threshold must be a float.")

    def set_prevalence_threshold_min(self, prevalence_threshold_min):
        if isinstance(prevalence_threshold_min, (int, float, np.integer, np.floating)) and 0 <= prevalence_threshold_min <= 100:
            self.prevalence_threshold_min = prevalence_threshold_min
        else:
            raise ValueError("The prevalence threshold must be an integer or a float between 0-100.")

    def set_prevalence_threshold_max(self, prevalence_threshold_max):
        if isinstance(prevalence_threshold_max, (int, float, np.integer, np.floating)) and 0 <= prevalence_threshold_max <= 100:
            self.prevalence_threshold_max = prevalence_threshold_max
        else:
            raise ValueError("The prevalence threshold must be an integer or a float between 0-100.")

    def set_subgroup(self, subgroup):
        if isinstance(subgroup, str):
            self.subgroup = subgroup
        else:
            raise ValueError("subgroup must be a string.")

    def get_attribute(self, attr_name):
        if hasattr(self, attr_name):
            return getattr(self, attr_name)
        else:
            raise AttributeError(f"Config has no attribute named '{attr_name}'")

    def get_encoding(self, target_df):
        if self.config.group_label_encoding is None:
            le = LabelEncoder()
            target = le.fit_transform(target_df)
            self.le_classes = dict(zip(le.classes_, range(len(le.classes_))))
            return pd.DataFrame({self.config.col_target: target}, index=target_df.index)
        else:
            return target_df.map(self.config.group_label_encoding)

    def get_target_df(self):
        target_df = self.metadata_handler.get_individuals_metadata_df()[self.config.col_target]

        try:
            # Try to convert the target column to integers
            target_df = target_df.astype(int)
            return target_df
        except ValueError:
            # If conversion fails, check for mixed values
            if target_df.apply(lambda x: isinstance(x, int)).any():
                raise ValueError(
                    "The target column must contain only the group test labels to encode or must already be encoded.")
            else:
                # If all are non-numeric, proceed to encoding
                return self.get_encoding(target_df)


    def filter_oligos_target_df(self, target_oligos_df):
        # Initialize the filters conditionally

        correlation_filter = CorrelationFilter(threshold=0.9, method='phi') if self.filter_by_correlation else None
        entropy_filter = EntropyFilter(threshold=self.entropy_threshold) if self.filter_by_entropy else None
        prevalence_filter = PrevalenceFilter(threshold_min=self.prevalence_threshold_min,
                                             threshold_max=self.prevalence_threshold_max)  # Always apply prevalence filter based on given threshold

        # Initialize the pipeline steps based on active filters
        pipeline_steps = []

        if correlation_filter is not None:
            pipeline_steps.append(('correlation_filter', correlation_filter))
        if entropy_filter is not None:
            pipeline_steps.append(('entropy_filter', entropy_filter))

        # Prevalence filter is always added
        pipeline_steps.append(('prevalence_filter', prevalence_filter))

        # Create a feature selection pipeline with active steps
        pipeline = Pipeline(pipeline_steps)

        # Fit the pipeline and transform the DataFrame
        return pipeline.fit_transform(target_oligos_df)

    def get_oligos_with_target(self):
        target_df = self.get_target_df()  # Get target data
        oligos_df = self.oligos_handler.get_oligos_df()  # Get peptide data

        # Merge target and oligos dataframes
        target_oligos_df = pd.merge(target_df,
                                    oligos_df,
                                    left_index=True,
                                    right_index=True,
                                    how='inner').set_index(self.config.col_target, append=True).reorder_levels([1, 0])
        target_oligos_df.index.rename([self.config.col_target, self.config.col_sample_name], inplace=True)

        # Apply filters if provided
        return self.filter_oligos_target_df(target_oligos_df)

    def get_oligos_metadata_subgroup_with_target(self):
        target_oligos_df = self.get_oligos_with_target()

        if self.subgroup != 'all':
            oligos_meta_df = self.oligos_handler.get_oligos_metadata_df()
            to_keep = oligos_meta_df[oligos_meta_df[self.subgroup].notna() & oligos_meta_df[self.subgroup].astype(bool)].index
            to_keep = to_keep[to_keep.isin(target_oligos_df.columns)]
            target_oligos_df = target_oligos_df[to_keep] #all rows and only columns with true value

        return target_oligos_df

    def get_oligos_additional_features_run_plates_with_target(self, target_with_without_oligos_df, **impute_kwargs):
        if self.with_additional_features and self.with_run_plates:
            ind_meta_df = self.metadata_handler.get_additional_features_run_plates_df( **impute_kwargs)
        elif self.with_additional_features:
            ind_meta_df = self.metadata_handler.get_additional_features_df(**impute_kwargs)
        elif self.with_run_plates:
            ind_meta_df = self.metadata_handler.get_run_plates_df()
        else:
            raise ValueError(
                "At least one of 'with_oligos', 'with_additional_features', or 'with_run_plates' must be True.")
            #return target_with_without_oligos_df

        if isinstance(target_with_without_oligos_df, pd.DataFrame):
            return pd.merge(target_with_without_oligos_df, ind_meta_df, left_index=True, right_index=True, how='inner')
        else: # without oligos
            return pd.merge(target_with_without_oligos_df, ind_meta_df, left_on=self.config.col_sample_name, right_on=self.config.col_sample_name).set_index(self.config.col_target, append=True).reorder_levels([1, 0])


    def get_data_with_target(self, **impute_kwargs):
        if self.with_oligos:
            target_oligos_df = self.get_oligos_metadata_subgroup_with_target()
            if self.with_additional_features or self.with_run_plates:
                return self.get_oligos_additional_features_run_plates_with_target(target_oligos_df, **impute_kwargs)
            else:
                return target_oligos_df

        elif self.with_additional_features or self.with_run_plates:
            target_df = self.get_target_df()
            return self.get_oligos_additional_features_run_plates_with_target(target_df, **impute_kwargs)
        else:
            raise ValueError(
                "At least one of 'with_oligos', 'with_additional_features', or 'with_run_plates' must be True.")
            #return pd.DataFrame()

    def get_category_oligos_with_target(self, target_oligos_df):
        oligos_meta_df = self.oligos_handler.get_oligos_metadata_df()

        # Filter the metadata DataFrame to include only the desired subgroups
        valid_subgroups = [subgroup for subgroup in self.config.subgroups_to_include if
                           subgroup in oligos_meta_df.columns]
        metadata_filtered = oligos_meta_df.loc[
            oligos_meta_df.index.intersection(target_oligos_df.columns), valid_subgroups]

        # Here we create a dictionary where each peptide maps to a list of subgroups it belongs to
        peptide_to_subgroups = metadata_filtered.apply(
            lambda row: [subgroup for subgroup in valid_subgroups if row[subgroup]], axis=1)

        # Initialize an empty DataFrame to store subgroup counts for each sample
        target_category_oligos_df = pd.DataFrame(index=target_oligos_df.index, columns=valid_subgroups, data=0)

        # Iterate over each sample in the multi-index DataFrame and calculate counts
        for (group_test, sample_id), sample_data in target_oligos_df.groupby(level=[0, 1]):
            # Get the list of peptides that are present in the sample
            present_peptides = sample_data.columns[sample_data.iloc[0] > 0]

            # Count presence in each subgroup for the sample
            for peptide in present_peptides:
                # Check which subgroups this peptide belongs to
                subgroups = peptide_to_subgroups.get(peptide, [])
                for subgroup in subgroups:
                    # Increment the count for the subgroup for this specific sample
                    target_category_oligos_df.at[(group_test, sample_id), subgroup] += 1

        entropy_filter = EntropyFilter(threshold=self.entropy_threshold)
        return entropy_filter.fit_transform(target_category_oligos_df)


    def get_aggregated_data_with_target(self, **impute_kwargs):
        target_oligos_df = self.get_oligos_with_target()
        target_category_oligos_df = self.get_category_oligos_with_target(target_oligos_df)

        if self.with_additional_features or self.with_run_plates:
            return self.get_oligos_additional_features_run_plates_with_target(target_category_oligos_df,
                                                                              **impute_kwargs)
        else:
            return target_category_oligos_df

    def split_features_target_df(self, target_features_df):
        target = target_features_df.reset_index(level=0)[self.config.col_target]
        features = target_features_df.reset_index(level=0, drop=True)

        if self.fillna:
            features = features.fillna(0).infer_objects(copy=False)
        if features.shape[1] == 0:
            return pd.DataFrame(), pd.Series()

        return features, target


    def get_features_target(self, **impute_kwargs):
        if self.group_oligos:
            target_features_df = self.get_aggregated_data_with_target(**impute_kwargs).sort_index(level=self.config.col_target)
        else:
            target_features_df = self.get_data_with_target(**impute_kwargs).sort_index(level=self.config.col_target)

        features, target = self.split_features_target_df(target_features_df)

        return features, target


